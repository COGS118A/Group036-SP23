{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d93ef05",
   "metadata": {},
   "source": [
    "# Neural Networks vs XGBoost for Regression vs Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow.feather as feather\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import xgboost as xgb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16414d",
   "metadata": {},
   "source": [
    "First, we organize all our data. All data has been preprocessed in this notebook: https://github.com/COGS118A/Group036-SP23/blob/main/Full_dataset_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lyrics = pd.read_feather('Full_Processed_Data/data_lyrics1_training').reset_index(drop=True).iloc[500000:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles = pd.read_feather('Full_Processed_Data/data_titles_training').reset_index(drop=True).iloc[500000:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_else = pd.read_feather('Full_Processed_Data/data_else_training').reset_index(drop=True).iloc[500000:1000000]\n",
    "df_artists = df_else['artist']\n",
    "df_tags = df_else[['tag0', 'tag1', 'tag2', 'tag3', 'tag4', 'tag5']]\n",
    "df_age = df_else['age']\n",
    "df_views = df_else['encoded_views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "view_ohe = ohe.fit_transform(df_views.values.reshape(-1, 1))\n",
    "df_views_ohe = pd.DataFrame(view_ohe.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62982c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_lyrics, df_titles, df_artists, df_age, df_tags, df_views, df_views_ohe]\n",
    "data = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up data into training and validation sets\n",
    "train_idx, test_idx = train_test_split(list(range(0, len(df_views))), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22543951",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12cbb5",
   "metadata": {},
   "source": [
    " ### Neural Networks for Regression vs Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features in tensor form for neural network (ensure we only use training data here) => Also push to cuda\n",
    "\n",
    "lyrics = torch.tensor(df_lyrics.loc[train_idx].values).type('torch.FloatTensor').to(device)\n",
    "titles = torch.tensor(df_titles.loc[train_idx].values).type('torch.FloatTensor').to(device)\n",
    "artists = torch.tensor(df_artists.loc[train_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)\n",
    "age = torch.tensor(df_age.loc[train_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)\n",
    "tags = torch.tensor(df_tags.loc[train_idx].values).type('torch.FloatTensor').to(device)\n",
    "views = torch.tensor(df_views.loc[train_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)\n",
    "#views_class = torch.tensor(df_views_ohe.loc[train_idx].values).type('torch.FloatTensor').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lyrics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the neural network architecture for Regression\n",
    "class SongViewCountPredictor1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SongViewCountPredictor1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sig(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the neural network architecture for Classification\n",
    "class SongViewCountPredictor2(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SongViewCountPredictor2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.soft = nn.functional.softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.soft(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ecfaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, lyrics, titles, artists, age, tags, views):\n",
    "        self.lyrics = lyrics\n",
    "        self.titles = titles\n",
    "        self.artists = artists\n",
    "        self.age = age\n",
    "        self.tags = tags\n",
    "        self.views = views\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.views)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lyrics = self.lyrics[index]\n",
    "        titles = self.titles[index]\n",
    "        artists = self.artists[index]\n",
    "        age = self.age[index]\n",
    "        tags = self.tags[index]\n",
    "        views = self.views[index]\n",
    "        return lyrics, titles, artists, age, tags, views\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "dataset = SongDataset(lyrics, titles, artists, age, tags, views)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the model and optimizer (push models to GPU)\n",
    "input_size = lyrics.shape[1] + titles.shape[1] + 1 + 1 + tags.shape[1] # (artists and age are size 1)\n",
    "model1 = SongViewCountPredictor1(input_size).to(device)\n",
    "#model2 = SongViewCountPredictor2(input_size).to(device)\n",
    "optimizer1 = optim.Adam(model1.parameters())\n",
    "#optimizer2 = optim.Adam(model2.parameters())\n",
    "criterion = nn.MSELoss() # using MSE as loss\n",
    "#criterion2 = nn.CrossEntropyLoss() # using cross entropy as loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c75e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss1 = 0.0\n",
    "    epoch_loss2 = 0.0\n",
    "    for lyrics, titles, artists, age, tags, views in dataloader:\n",
    "        # move batch data to GPU\n",
    "        lyrics = lyrics.to(device)\n",
    "        titles = titles.to(device)\n",
    "        artists = artists.to(device)\n",
    "        age = age.to(device)\n",
    "        tags = tags.to(device)\n",
    "        views = views.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        inputs = torch.cat((lyrics, titles, artists, age, tags), dim=1)\n",
    "        outputs1 = model1(inputs)\n",
    "        #outputs2 = model2(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss1 = criterion(outputs1, views)\n",
    "        #loss2 = criterion(outputs2, views)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        #optimizer2.zero_grad()\n",
    "        #loss2.backward()\n",
    "        #optimizer2.step()\n",
    "        \n",
    "        epoch_loss1 += loss1.item()\n",
    "        #epoch_loss2 += loss2.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]:\\n Loss1: {epoch_loss1/len(dataloader):.4f} \\n Loss2: {epoch_loss2/len(dataloader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5977f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform testing data into tensors (ensure we're only locating testing data)\n",
    "\n",
    "lyrics = torch.tensor(df_lyrics.loc[test_idx].values).type('torch.FloatTensor').to(device)\n",
    "titles = torch.tensor(df_titles.loc[test_idx].values).type('torch.FloatTensor').to(device)\n",
    "artists = torch.tensor(df_artists.loc[test_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)\n",
    "age = torch.tensor(df_age.loc[test_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)\n",
    "tags = torch.tensor(df_tags.loc[test_idx].values).type('torch.FloatTensor').to(device)\n",
    "views = torch.tensor(df_views.loc[test_idx].values).unsqueeze(1).type('torch.FloatTensor').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "# Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "model1.eval()\n",
    "#model2.eval()\n",
    "\n",
    "dataset_test = SongDataset(lyrics, titles, artists, age, tags, views)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n",
    "\n",
    "size = len(dataloader_test.dataset)\n",
    "num_batches = len(dataloader_test)\n",
    "\n",
    "test_loss1, correct1 = 0, 0\n",
    "test_loss2, correct2 = 0, 0\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "mse_vals = []\n",
    "epochs = []\n",
    "\n",
    "# Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "# also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "with torch.no_grad():\n",
    "    for lyrics, titles, artists, age, tags, views in dataloader_test:\n",
    "        # move batch data to GPU\n",
    "        lyrics = lyrics.to(device)\n",
    "        titles = titles.to(device)\n",
    "        artists = artists.to(device)\n",
    "        age = age.to(device)\n",
    "        tags = tags.to(device)\n",
    "        views = views.to(device)\n",
    "        \n",
    "        # predict the views count\n",
    "        X = torch.cat((lyrics, titles, artists, age, tags), dim=1)\n",
    "        pred1 = model1(X)\n",
    "        #pred2 = model2(X)\n",
    "        \n",
    "        y = views\n",
    "        test_loss1 += loss_fn(pred1, y).item()\n",
    "        correct1 += (abs(pred1.argmax(1) - y) < 1000).type(torch.float).sum().item()\n",
    "        \n",
    "        all_predictions.extend(pred1.argmax(1).cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        \n",
    "        # Calculate the MSE and store it\n",
    "        mse = loss.item()\n",
    "        mse_values.append(mse)\n",
    "        epochs.append(epoch)\n",
    "        \n",
    "        #test_loss2 += loss_fn(pred2, y).item()\n",
    "        #correct2 += (pred2.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "test_loss1 /= num_batches\n",
    "correct1 /= size\n",
    "\n",
    "#test_loss2 /= num_batches\n",
    "#correct2 /= size\n",
    "\n",
    "print(f\"Test Error 1: \\n Accuracy: {(100*correct1):>0.1f}%, Avg loss: {test_loss1:>8f} \\n Test Error 2: \\n Accuracy: {(100*correct2):>0.1f}%, Avg loss: {test_loss2:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of observed vs predicted values\n",
    "plt.scatter(rf_target_test, rf_predictions, alpha=0.5)\n",
    "plt.title('Scatter plot of Observed vs Predicted Values')\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05154816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MSE values over epochs\n",
    "plt.plot(epochs, mse_values)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e78957",
   "metadata": {},
   "source": [
    "### XGBoost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost classifier using random search and the gpu\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10], \n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "xgb = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, \n",
    "                        objective='multi:softmax', nthread=1, \n",
    "                        tree_method = 'gpu_hist', verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, \n",
    "                                   n_iter=param_comb, scoring='accuracy', \n",
    "                                   n_jobs=1, cv=kf.split(data.iloc[train_idx], \n",
    "                                                         df_views.iloc[train_idx]), verbose=3, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(data.iloc[train_idx], df_views.iloc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accuracy\n",
    "accuracy_score(df_views.iloc[test_idx], random_search.predict(data.iloc[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train best model on all training data\n",
    "xgb_model = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, \n",
    "                              objective='multi:softmax', nthread=1, \n",
    "                              tree_method = 'gpu_hist', verbosity=3, \n",
    "                              min_child_weight=10, gamma=1.5, subsample=0.8, \n",
    "                              colsample_bytree=0.6, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(data.iloc[train_idx], df_views.iloc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f794e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model \n",
    "accuracy_score(df_views.iloc[test_idx], xgb_model.predict(data.iloc[test_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6872e7",
   "metadata": {},
   "source": [
    "### XGBoost for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec336b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost classifier using random search and the gpu\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10], \n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "xgb = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, \n",
    "                        objective='multi:softmax', nthread=1, \n",
    "                        tree_method = 'gpu_hist', verbosity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 3\n",
    "param_comb = 5\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb, param_distributions=params, \n",
    "                                   n_iter=param_comb, scoring='accuracy', \n",
    "                                   n_jobs=1, cv=kf.split(data.iloc[train_idx], \n",
    "                                                         df_views.iloc[train_idx]), verbose=3, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46325761",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(data.iloc[train_idx], df_views.iloc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8591e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best parameters\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accuracy\n",
    "accuracy_score(df_views.iloc[test_idx], random_search.predict(data.iloc[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6597031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train best model on all training data\n",
    "xgb_model = xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, \n",
    "                              objective='multi:softmax', nthread=1, \n",
    "                              tree_method = 'gpu_hist', verbosity=3, \n",
    "                              min_child_weight=10, gamma=1.5, subsample=0.8, \n",
    "                              colsample_bytree=0.6, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfeb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(data.iloc[train_idx], df_views.iloc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model \n",
    "accuracy_score(df_views.iloc[test_idx], xgb_model.predict(data.iloc[test_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
